{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233768d9-beb1-467c-9b16-cc42da9b8656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "377140e2-cc5a-4087-a376-2e8de7c3e969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bert-score in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (2.7.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (2.3.1)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (4.55.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (3.10.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.0.0->bert-score) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.0.0->bert-score) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=3.0.0->bert-score) (0.34.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=3.0.0->bert-score) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->bert-score) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->bert-score) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->bert-score) (3.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->bert-score) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from requests->bert-score) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bert-score --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf5560f-27ae-4d0a-b373-a15fa5bf3399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba48348185246cb95be9d90b1b82d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caba428b77f4d35a0f0fb926ef82cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e23d4d613149a5adbda2c84510903d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.98 seconds, 0.34 sentences/sec\n",
      "0.9640105366706848\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "cands = [\"The cat is on the mat.\"]\n",
    "refs = [\"A cat is sitting on the mat.\"]\n",
    "P, R, F1 = score(cands, refs, lang=\"en\", verbose=True)\n",
    "print(F1.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36239676-9b7c-4ee3-b575-973b2775026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.19.0\n",
      "Uninstalling tensorflow-2.19.0:\n",
      "  Successfully uninstalled tensorflow-2.19.0\n",
      "Found existing installation: tensorflow-intel 2.16.1\n",
      "Uninstalling tensorflow-intel-2.16.1:\n",
      "  Successfully uninstalled tensorflow-intel-2.16.1\n",
      "Found existing installation: keras 3.9.2\n",
      "Uninstalling keras-3.9.2:\n",
      "  Successfully uninstalled keras-3.9.2\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall tensorflow tensorflow-intel keras -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4516b879-f4fa-49b5-857f-7094f74b2edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Summary Evaluation System Demo...\n",
      "============================================================\n",
      "Starting demonstration of summary evaluation system...\n",
      "Initializing evaluation models...\n",
      "Loading DeBERTa model for BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ab5aaf93fd4a8e9fa007956e2718a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   2%|1         | 52.4M/3.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/microsoft/deberta-xlarge-mnli/resolve/main/pytorch_model.bin: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://huggingface.co/microsoft/deberta-xlarge-mnli/resolve/main/pytorch_model.bin: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa model failed, using fallback: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/deberta-xlarge-mnli/resolve/main/pytorch_model.bin (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002120E9A8920>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3f0bf337-d1af-4460-8aba-ce6ba5ef28d2)')\n",
      "Error during demo: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/distilbert-base-uncased/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002120E9A90D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: e26247f2-8c44-4585-8264-c4510078ec1c)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py\", line 779, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py\", line 904, in _raw_read\n",
      "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py\", line 887, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\http\\client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py\", line 820, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py\", line 1091, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py\", line 980, in read\n",
      "    data = self._raw_read(amt)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py\", line 903, in _raw_read\n",
      "    with self._error_catcher():\n",
      "  File \"C:\\Python312\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py\", line 784, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py\", line 496, in http_get\n",
      "    for chunk in r.iter_content(chunk_size=constants.DOWNLOAD_CHUNK_SIZE):\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py\", line 826, in generate\n",
      "    raise ConnectionError(e)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 976, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n",
      "    raise new_e\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py\", line 753, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py\", line 205, in _new_conn\n",
      "    raise NameResolutionError(self.host, self, e) from e\n",
      "urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x000002120E9A8920>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-xlarge-mnli/resolve/main/pytorch_model.bin (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002120E9A8920>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18764\\3367008783.py\", line 37, in __init__\n",
      "    self.bert_scorer_en = BERTScorer(\n",
      "                          ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\bert_score\\scorer.py\", line 98, in __init__\n",
      "    self._model = get_model(self.model_type, self.num_layers, self.all_layers)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\bert_score\\utils.py\", line 255, in get_model\n",
      "    model = AutoModel.from_pretrained(model_type)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 600, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 317, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 4918, in from_pretrained\n",
      "    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 1208, in _get_resolved_checkpoint_files\n",
      "    resolved_archive_file = cached_file(\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py\", line 321, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py\", line 567, in cached_files\n",
      "    raise e\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py\", line 479, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py\", line 1010, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py\", line 1171, in _hf_hub_download_to_cache_dir\n",
      "    _download_to_tmp_and_move(\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py\", line 1738, in _download_to_tmp_and_move\n",
      "    http_get(\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py\", line 513, in http_get\n",
      "    return http_get(\n",
      "           ^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py\", line 426, in http_get\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py\", line 309, in _request_wrapper\n",
      "    response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(429,))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_http.py\", line 310, in http_backoff\n",
      "    response = session.request(method=method, url=url, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/deberta-xlarge-mnli/resolve/main/pytorch_model.bin (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002120E9A8920>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3f0bf337-d1af-4460-8aba-ce6ba5ef28d2)')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\socket.py\", line 976, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 488, in _make_request\n",
      "    raise new_e\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 464, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 1093, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py\", line 753, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py\", line 205, in _new_conn\n",
      "    raise NameResolutionError(self.host, self, e) from e\n",
      "urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x000002120E9A90D0>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/distilbert-base-uncased/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002120E9A90D0>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18764\\3367008783.py\", line 534, in <module>\n",
      "    evaluator, results_df = demo_evaluation()\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18764\\3367008783.py\", line 504, in demo_evaluation\n",
      "    evaluator = SummaryEvaluator()\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18764\\3367008783.py\", line 44, in __init__\n",
      "    self.bert_scorer_en = BERTScorer(\n",
      "                          ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\bert_score\\scorer.py\", line 97, in __init__\n",
      "    self._tokenizer = get_tokenizer(self.model_type, self._use_fast_tokenizer)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\bert_score\\utils.py\", line 329, in get_tokenizer\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_type, use_fast=use_fast)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 1138, in from_pretrained\n",
      "    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py\", line 2012, in from_pretrained\n",
      "    for template in list_repo_templates(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py\", line 167, in list_repo_templates\n",
      "    return [\n",
      "           ^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\hf_api.py\", line 3171, in list_repo_tree\n",
      "    for path_info in paginate(path=tree_url, headers=headers, params={\"recursive\": recursive, \"expand\": expand}):\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_pagination.py\", line 36, in paginate\n",
      "    r = session.get(path, params=params, headers=headers)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py\", line 602, in get\n",
      "    return self.request(\"GET\", url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_http.py\", line 96, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/distilbert-base-uncased/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002120E9A90D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: e26247f2-8c44-4585-8264-c4510078ec1c)')\n"
     ]
    }
   ],
   "source": [
    "# Fixed Summary Evaluation System - No Circular Imports\n",
    "# Save this as: summary_evaluation_system.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class SummaryEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive summary evaluation system using ROUGE and BERTScore metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initializing evaluation models...\")\n",
    "        \n",
    "        # Initialize ROUGE scorer with multiple metrics\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'], \n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Initialize BERTScore with different models for robustness\n",
    "        try:\n",
    "            print(\"Loading DeBERTa model for BERTScore...\")\n",
    "            self.bert_scorer_en = BERTScorer(\n",
    "                model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "                lang=\"en\",\n",
    "                rescale_with_baseline=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"DeBERTa model failed, using fallback: {e}\")\n",
    "            self.bert_scorer_en = BERTScorer(\n",
    "                model_type=\"distilbert-base-uncased\",\n",
    "                lang=\"en\",\n",
    "                rescale_with_baseline=True\n",
    "            )\n",
    "        \n",
    "        # Alternative BERTScore model for comparison\n",
    "        try:\n",
    "            print(\"Loading RoBERTa model for BERTScore...\")\n",
    "            self.bert_scorer_roberta = BERTScorer(\n",
    "                model_type=\"roberta-large\",\n",
    "                lang=\"en\", \n",
    "                rescale_with_baseline=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"RoBERTa model failed, using same as primary: {e}\")\n",
    "            self.bert_scorer_roberta = self.bert_scorer_en\n",
    "        \n",
    "        self.evaluation_results = []\n",
    "        print(\"Evaluator initialized successfully!\")\n",
    "        \n",
    "    def calculate_rouge_scores(self, generated_summary, reference_summary):\n",
    "        \"\"\"\n",
    "        Calculate ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "        \"\"\"\n",
    "        scores = self.rouge_scorer.score(reference_summary, generated_summary)\n",
    "        \n",
    "        rouge_metrics = {\n",
    "            'rouge1_precision': scores['rouge1'].precision,\n",
    "            'rouge1_recall': scores['rouge1'].recall,\n",
    "            'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "            'rouge2_precision': scores['rouge2'].precision,\n",
    "            'rouge2_recall': scores['rouge2'].recall,\n",
    "            'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "            'rougeL_precision': scores['rougeL'].precision,\n",
    "            'rougeL_recall': scores['rougeL'].recall,\n",
    "            'rougeL_f1': scores['rougeL'].fmeasure,\n",
    "        }\n",
    "        \n",
    "        return rouge_metrics\n",
    "    \n",
    "    def calculate_bert_scores(self, generated_summary, reference_summary):\n",
    "        \"\"\"\n",
    "        Calculate BERTScore using multiple models\n",
    "        \"\"\"\n",
    "        # Primary BERTScore (DeBERTa)\n",
    "        P1, R1, F1_1 = self.bert_scorer_en.score([generated_summary], [reference_summary])\n",
    "        \n",
    "        # Secondary BERTScore (RoBERTa) \n",
    "        P2, R2, F1_2 = self.bert_scorer_roberta.score([generated_summary], [reference_summary])\n",
    "        \n",
    "        bert_metrics = {\n",
    "            'bert_precision_deberta': P1.item(),\n",
    "            'bert_recall_deberta': R1.item(), \n",
    "            'bert_f1_deberta': F1_1.item(),\n",
    "            'bert_precision_roberta': P2.item(),\n",
    "            'bert_recall_roberta': R2.item(),\n",
    "            'bert_f1_roberta': F1_2.item(),\n",
    "            'bert_avg_precision': (P1.item() + P2.item()) / 2,\n",
    "            'bert_avg_recall': (R1.item() + R2.item()) / 2,\n",
    "            'bert_avg_f1': (F1_1.item() + F1_2.item()) / 2\n",
    "        }\n",
    "        \n",
    "        return bert_metrics\n",
    "    \n",
    "    def calculate_consistency_score(self, generated_summary, reference_summary):\n",
    "        \"\"\"\n",
    "        Calculate consistency score based on overlapping content\n",
    "        \"\"\"\n",
    "        gen_words = set(generated_summary.lower().split())\n",
    "        ref_words = set(reference_summary.lower().split())\n",
    "        \n",
    "        if len(ref_words) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        overlap = len(gen_words.intersection(ref_words))\n",
    "        consistency = overlap / len(ref_words)\n",
    "        \n",
    "        return consistency\n",
    "    \n",
    "    def calculate_relevancy_score(self, generated_summary, reference_summary):\n",
    "        \"\"\"\n",
    "        Calculate relevancy using semantic similarity approach\n",
    "        \"\"\"\n",
    "        gen_words = set(generated_summary.lower().split())\n",
    "        ref_words = set(reference_summary.lower().split())\n",
    "        \n",
    "        if len(gen_words.union(ref_words)) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        jaccard_similarity = len(gen_words.intersection(ref_words)) / len(gen_words.union(ref_words))\n",
    "        return jaccard_similarity\n",
    "    \n",
    "    def calculate_accuracy_score(self, generated_summary, reference_summary):\n",
    "        \"\"\"\n",
    "        Calculate accuracy as a combination of ROUGE-L and BERTScore F1\n",
    "        \"\"\"\n",
    "        rouge_scores = self.calculate_rouge_scores(generated_summary, reference_summary)\n",
    "        bert_scores = self.calculate_bert_scores(generated_summary, reference_summary)\n",
    "        \n",
    "        # Weighted combination of ROUGE-L F1 and BERTScore F1\n",
    "        accuracy = (0.4 * rouge_scores['rougeL_f1'] + \n",
    "                   0.6 * bert_scores['bert_avg_f1'])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def evaluate_summary(self, generated_summary, reference_summary, summary_type, document_id=None):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of a single summary\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating {summary_type} summary...\")\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        rouge_metrics = self.calculate_rouge_scores(generated_summary, reference_summary)\n",
    "        bert_metrics = self.calculate_bert_scores(generated_summary, reference_summary)\n",
    "        \n",
    "        consistency = self.calculate_consistency_score(generated_summary, reference_summary)\n",
    "        relevancy = self.calculate_relevancy_score(generated_summary, reference_summary)\n",
    "        accuracy = self.calculate_accuracy_score(generated_summary, reference_summary)\n",
    "        \n",
    "        # Combine all metrics\n",
    "        evaluation_result = {\n",
    "            'document_id': document_id or f\"doc_{len(self.evaluation_results)}\",\n",
    "            'summary_type': summary_type,\n",
    "            'generated_summary': generated_summary,\n",
    "            'reference_summary': reference_summary,\n",
    "            'consistency': consistency,\n",
    "            'relevancy': relevancy,\n",
    "            'accuracy': accuracy,\n",
    "            **rouge_metrics,\n",
    "            **bert_metrics\n",
    "        }\n",
    "        \n",
    "        self.evaluation_results.append(evaluation_result)\n",
    "        print(f\"Completed evaluation for {summary_type}\")\n",
    "        return evaluation_result\n",
    "    \n",
    "    def evaluate_multiple_summaries(self, summary_data):\n",
    "        \"\"\"\n",
    "        Evaluate multiple summaries and summary types\n",
    "        \n",
    "        summary_data format:\n",
    "        [\n",
    "            {\n",
    "                'document_id': 'doc_1',\n",
    "                'generated_summary': '...',\n",
    "                'reference_summary': '...',\n",
    "                'summary_type': 'brief'\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        print(f\"Starting batch evaluation of {len(summary_data)} summaries...\")\n",
    "        results = []\n",
    "        \n",
    "        for i, data in enumerate(summary_data):\n",
    "            print(f\"Processing summary {i+1}/{len(summary_data)}\")\n",
    "            result = self.evaluate_summary(\n",
    "                generated_summary=data['generated_summary'],\n",
    "                reference_summary=data['reference_summary'],\n",
    "                summary_type=data['summary_type'],\n",
    "                document_id=data.get('document_id', f\"doc_{len(results)}\")\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "        print(\"Batch evaluation completed!\")\n",
    "        return results\n",
    "    \n",
    "    def get_results_dataframe(self):\n",
    "        \"\"\"\n",
    "        Convert results to pandas DataFrame for analysis\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(self.evaluation_results)\n",
    "    \n",
    "    def create_comprehensive_visualization(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization dashboard\n",
    "        \"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"No evaluation results available. Please run evaluations first.\")\n",
    "            return\n",
    "            \n",
    "        df = self.get_results_dataframe()\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        fig.suptitle('Summary Evaluation Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # 1. Overall Performance Heatmap\n",
    "        plt.subplot(3, 4, 1)\n",
    "        metrics_cols = ['consistency', 'relevancy', 'accuracy', 'rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'bert_avg_f1']\n",
    "        heatmap_data = df.groupby('summary_type')[metrics_cols].mean()\n",
    "        sns.heatmap(heatmap_data.T, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "        plt.title('Performance Heatmap by Summary Type', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Metrics')\n",
    "        \n",
    "        # 2. ROUGE Scores Comparison\n",
    "        plt.subplot(3, 4, 2)\n",
    "        rouge_metrics = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']\n",
    "        rouge_data = df.groupby('summary_type')[rouge_metrics].mean()\n",
    "        rouge_data.plot(kind='bar', ax=plt.gca())\n",
    "        plt.title('ROUGE Scores by Summary Type', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # 3. BERTScore Comparison  \n",
    "        plt.subplot(3, 4, 3)\n",
    "        bert_metrics = ['bert_avg_precision', 'bert_avg_recall', 'bert_avg_f1']\n",
    "        bert_data = df.groupby('summary_type')[bert_metrics].mean()\n",
    "        bert_data.plot(kind='bar', ax=plt.gca(), color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "        plt.title('BERTScore by Summary Type', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # 4. Custom Metrics (Consistency, Relevancy, Accuracy)\n",
    "        plt.subplot(3, 4, 4)\n",
    "        custom_metrics = ['consistency', 'relevancy', 'accuracy']\n",
    "        custom_data = df.groupby('summary_type')[custom_metrics].mean()\n",
    "        custom_data.plot(kind='bar', ax=plt.gca(), color=['gold', 'mediumpurple', 'tomato'])\n",
    "        plt.title('Custom Evaluation Metrics', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # 5. Precision vs Recall Scatter\n",
    "        plt.subplot(3, 4, 5)\n",
    "        summary_types = df['summary_type'].unique()\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(summary_types)))\n",
    "        \n",
    "        for i, summary_type in enumerate(summary_types):\n",
    "            subset = df[df['summary_type'] == summary_type]\n",
    "            plt.scatter(subset['rouge1_recall'], subset['rouge1_precision'], \n",
    "                       label=summary_type, alpha=0.7, s=100, color=colors[i])\n",
    "        plt.xlabel('ROUGE-1 Recall')\n",
    "        plt.ylabel('ROUGE-1 Precision')\n",
    "        plt.title('Precision vs Recall (ROUGE-1)', fontsize=12, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Overall Performance Ranking\n",
    "        plt.subplot(3, 4, 6)\n",
    "        df['overall_score'] = (0.25 * df['consistency'] + 0.25 * df['relevancy'] + \n",
    "                              0.25 * df['accuracy'] + 0.25 * df['bert_avg_f1'])\n",
    "        \n",
    "        overall_ranking = df.groupby('summary_type')['overall_score'].mean().sort_values(ascending=False)\n",
    "        bars = plt.bar(range(len(overall_ranking)), overall_ranking.values, \n",
    "                      color=plt.cm.viridis(np.linspace(0, 1, len(overall_ranking))))\n",
    "        plt.title('Overall Performance Ranking', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Summary Type')\n",
    "        plt.ylabel('Overall Score')\n",
    "        plt.xticks(range(len(overall_ranking)), overall_ranking.index, rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, overall_ranking.values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 7. Distribution of Scores\n",
    "        plt.subplot(3, 4, 7)\n",
    "        df.boxplot(column='accuracy', by='summary_type', ax=plt.gca())\n",
    "        plt.title('Accuracy Score Distribution', fontsize=12, fontweight='bold')\n",
    "        plt.suptitle('')  # Remove default title\n",
    "        plt.ylabel('Accuracy Score')\n",
    "        \n",
    "        # 8. Correlation Matrix\n",
    "        plt.subplot(3, 4, 8)\n",
    "        correlation_cols = ['consistency', 'relevancy', 'accuracy', 'rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'bert_avg_f1']\n",
    "        correlation_matrix = df[correlation_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "        plt.title('Metrics Correlation Matrix', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 9. BERTScore Model Comparison\n",
    "        plt.subplot(3, 4, 9)\n",
    "        bert_comparison = df[['summary_type', 'bert_f1_deberta', 'bert_f1_roberta']].groupby('summary_type').mean()\n",
    "        bert_comparison.plot(kind='bar', ax=plt.gca())\n",
    "        plt.title('BERTScore: DeBERTa vs RoBERTa', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(['DeBERTa', 'RoBERTa'])\n",
    "        \n",
    "        # 10. Performance Trends (if multiple documents)\n",
    "        plt.subplot(3, 4, 10)\n",
    "        if len(df['document_id'].unique()) > 1:\n",
    "            trend_data = df.groupby(['document_id', 'summary_type'])['accuracy'].mean().unstack(fill_value=0)\n",
    "            trend_data.plot(ax=plt.gca(), marker='o')\n",
    "            plt.title('Accuracy Trends Across Documents', fontsize=12, fontweight='bold')\n",
    "            plt.xlabel('Document ID')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend(title='Summary Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        else:\n",
    "            # Show score breakdown instead\n",
    "            score_breakdown = df.groupby('summary_type')[['rouge1_f1', 'rouge2_f1', 'bert_avg_f1']].mean()\n",
    "            score_breakdown.plot(kind='area', ax=plt.gca(), alpha=0.7)\n",
    "            plt.title('Score Composition by Type', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # 11. Summary Statistics Table\n",
    "        plt.subplot(3, 4, 11)\n",
    "        plt.axis('off')\n",
    "        summary_stats = df.groupby('summary_type')[['consistency', 'relevancy', 'accuracy', 'rouge1_f1', 'bert_avg_f1']].mean().round(3)\n",
    "        \n",
    "        # Create table\n",
    "        table_data = []\n",
    "        for summary_type in summary_stats.index:\n",
    "            row = [summary_type]\n",
    "            for col in summary_stats.columns:\n",
    "                row.append(f\"{summary_stats.loc[summary_type, col]:.3f}\")\n",
    "            table_data.append(row)\n",
    "        \n",
    "        table = plt.table(cellText=table_data,\n",
    "                         colLabels=['Type', 'Consistency', 'Relevancy', 'Accuracy', 'ROUGE-1', 'BERTScore'],\n",
    "                         cellLoc='center',\n",
    "                         loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1.2, 1.5)\n",
    "        plt.title('Performance Summary (Mean Scores)', fontsize=12, fontweight='bold', pad=20)\n",
    "        \n",
    "        # 12. Best vs Worst Comparison\n",
    "        plt.subplot(3, 4, 12)\n",
    "        best_type = overall_ranking.index[0]\n",
    "        worst_type = overall_ranking.index[-1] if len(overall_ranking) > 1 else best_type\n",
    "        \n",
    "        if best_type != worst_type:\n",
    "            comparison_data = df[df['summary_type'].isin([best_type, worst_type])]\n",
    "            comparison_means = comparison_data.groupby('summary_type')[['consistency', 'relevancy', 'accuracy']].mean()\n",
    "            comparison_means.plot(kind='bar', ax=plt.gca())\n",
    "            plt.title(f'Best vs Worst Performance\\n({best_type} vs {worst_type})', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel('Score')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend()\n",
    "        else:\n",
    "            # Single type - show metric breakdown\n",
    "            single_type_data = df[df['summary_type'] == best_type][['consistency', 'relevancy', 'accuracy']].mean()\n",
    "            single_type_data.plot(kind='bar', ax=plt.gca(), color='skyblue')\n",
    "            plt.title(f'Metric Breakdown: {best_type}', fontsize=12, fontweight='bold')\n",
    "            plt.ylabel('Score')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Visualization saved to {save_path}\")\n",
    "            \n",
    "        plt.show()\n",
    "    \n",
    "    def generate_detailed_report(self):\n",
    "        \"\"\"\n",
    "        Generate detailed evaluation report\n",
    "        \"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"No evaluation results available.\")\n",
    "            return None\n",
    "            \n",
    "        df = self.get_results_dataframe()\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"COMPREHENSIVE SUMMARY EVALUATION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Timestamp\n",
    "        print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Total summaries evaluated: {len(df)}\")\n",
    "        print(f\"Summary types: {', '.join(df['summary_type'].unique())}\")\n",
    "        print()\n",
    "        \n",
    "        # Overall Statistics\n",
    "        print(\"OVERALL STATISTICS\")\n",
    "        print(\"-\"*50)\n",
    "        overall_stats = df[['consistency', 'relevancy', 'accuracy', 'rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'bert_avg_f1']].describe()\n",
    "        print(overall_stats.round(4))\n",
    "        print()\n",
    "        \n",
    "        # Performance by Summary Type\n",
    "        print(\"PERFORMANCE BY SUMMARY TYPE\")\n",
    "        print(\"-\"*50)\n",
    "        type_performance = df.groupby('summary_type')[['consistency', 'relevancy', 'accuracy', 'rouge1_f1', 'bert_avg_f1']].mean()\n",
    "        print(type_performance.round(4))\n",
    "        print()\n",
    "        \n",
    "        # Best Performing Summary Type\n",
    "        print(\"BEST PERFORMING SUMMARY TYPE\")\n",
    "        print(\"-\"*50)\n",
    "        df['overall_score'] = (0.25 * df['consistency'] + 0.25 * df['relevancy'] + \n",
    "                              0.25 * df['accuracy'] + 0.25 * df['bert_avg_f1'])\n",
    "        best_type = df.groupby('summary_type')['overall_score'].mean().idxmax()\n",
    "        best_score = df.groupby('summary_type')['overall_score'].mean().max()\n",
    "        \n",
    "        print(f\"Best Summary Type: {best_type}\")\n",
    "        print(f\"Overall Score: {best_score:.4f}\")\n",
    "        \n",
    "        # Detailed Metrics Breakdown\n",
    "        print(f\"\\nDetailed metrics for '{best_type.upper()}':\")\n",
    "        print(\"-\"*30)\n",
    "        best_metrics = df[df['summary_type'] == best_type][['consistency', 'relevancy', 'accuracy', 'rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'bert_avg_f1']].mean()\n",
    "        \n",
    "        for metric, value in best_metrics.items():\n",
    "            print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nRECOMMENDATIONS\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        type_scores = df.groupby('summary_type')[['consistency', 'relevancy', 'accuracy', 'rouge1_f1', 'bert_avg_f1']].mean()\n",
    "        \n",
    "        for summary_type in type_scores.index:\n",
    "            print(f\"\\n{summary_type.upper()} Summary:\")\n",
    "            scores = type_scores.loc[summary_type]\n",
    "            \n",
    "            # Find strengths and weaknesses\n",
    "            strengths = scores.nlargest(2).index.tolist()\n",
    "            weaknesses = scores.nsmallest(2).index.tolist()\n",
    "            \n",
    "            print(f\"  Strengths: {', '.join([s.replace('_', ' ').title() for s in strengths])}\")\n",
    "            print(f\"  Areas for improvement: {', '.join([w.replace('_', ' ').title() for w in weaknesses])}\")\n",
    "        \n",
    "        print(f\"\\nOverall recommendation: Focus on improving '{best_type}' approach\")\n",
    "        print(\"as it shows the best performance across all metrics.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "def demo_evaluation():\n",
    "    \"\"\"\n",
    "    Demonstration of the evaluation system\n",
    "    \"\"\"\n",
    "    print(\"Starting demonstration of summary evaluation system...\")\n",
    "    \n",
    "    # Create test directories\n",
    "    os.makedirs('test', exist_ok=True)\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    # Sample data for testing\n",
    "    sample_summaries = [\n",
    "        {\n",
    "            'document_id': 'medical_doc_1',\n",
    "            'summary_type': 'brief',\n",
    "            'generated_summary': 'Patient has iron deficiency anaemia and beta thalassaemia trait. Blood tests show microcytic anaemia. Treatment includes iron supplementation.',\n",
    "            'reference_summary': 'The patient was diagnosed with iron deficiency anaemia and beta thalassaemia trait based on blood test results showing microcytic anaemia. The treatment plan includes iron supplementation and follow-up monitoring.'\n",
    "        },\n",
    "        {\n",
    "            'document_id': 'medical_doc_1', \n",
    "            'summary_type': 'detailed',\n",
    "            'generated_summary': 'Patient K B Chunchaiah, age 45, was admitted with iron deficiency anaemia and beta thalassaemia trait. Laboratory results revealed microcytic anaemia with elevated Mentzer index. The patient presented with symptoms of fatigue and pallor. Current treatment includes ferrous sulfate 200mg daily with planned follow-up in 2 months to monitor hemoglobin levels.',\n",
    "            'reference_summary': 'Patient K B Chunchaiah, 45 years old, was admitted to hospital with iron deficiency anaemia and beta thalassaemia trait. Blood tests showed microcytic anaemia with elevated Mentzer index. The patient has a history of fatigue and pallor. Current medications include Ferrous Sulfate 200mg daily. Treatment plan includes continuing iron supplementation and follow-up in 2 months with hemoglobin monitoring.'\n",
    "        },\n",
    "        {\n",
    "            'document_id': 'medical_doc_1',\n",
    "            'summary_type': 'domain_specific', \n",
    "            'generated_summary': 'Medical diagnosis: Iron deficiency anaemia with beta thalassaemia trait. Key findings: Microcytic anaemia, elevated Mentzer index (>13), symptoms of fatigue and pallor. Treatment protocol: Ferrous sulfate supplementation with 2-month follow-up schedule. Diagnostic note: HbA2 estimation remains gold standard for beta thalassaemia trait diagnosis.',\n",
    "            'reference_summary': 'Primary diagnosis of iron deficiency anaemia complicated by beta thalassaemia trait in 45-year-old male patient. Laboratory findings include microcytic anaemia with Mentzer index >13. Clinical presentation includes fatigue and pallor. Treatment regimen consists of ferrous sulfate 200mg daily with scheduled follow-up in 2 months for hemoglobin monitoring. Note that HbA2 estimation is the gold standard for diagnosing beta thalassaemia trait.'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = SummaryEvaluator()\n",
    "    \n",
    "    print(f\"Starting evaluation process...\")\n",
    "    \n",
    "    # Evaluate summaries\n",
    "    results = evaluator.evaluate_multiple_summaries(sample_summaries)\n",
    "    \n",
    "    print(f\"Evaluated {len(results)} summaries\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"Creating comprehensive visualizations...\")\n",
    "    evaluator.create_comprehensive_visualization(save_path='test/summary_evaluation_dashboard.png')\n",
    "    \n",
    "    # Generate detailed report\n",
    "    print(\"Generating detailed report...\")\n",
    "    df_results = evaluator.generate_detailed_report()\n",
    "    \n",
    "    # Save results to CSV\n",
    "    df_results.to_csv('test/evaluation_results.csv', index=False)\n",
    "    print(\"Results saved to test/evaluation_results.csv\")\n",
    "    \n",
    "    return evaluator, df_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the demonstration\n",
    "    print(\"Running Summary Evaluation System Demo...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        evaluator, results_df = demo_evaluation()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION SYSTEM DEMO COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Files created:\")\n",
    "        print(\"  test/summary_evaluation_dashboard.png\")\n",
    "        print(\"  test/evaluation_results.csv\")\n",
    "        print()\n",
    "        print(\"The evaluation system is ready for use!\")\n",
    "        print(\"You can now use:\")\n",
    "        print(\"  evaluator.evaluate_summary() - for single summaries\")\n",
    "        print(\"  evaluator.evaluate_multiple_summaries() - for batch evaluation\")\n",
    "        print(\"  evaluator.create_comprehensive_visualization() - for charts\")\n",
    "        print(\"  evaluator.generate_detailed_report() - for detailed analysis\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during demo: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f681d-4e55-4e98-99fd-485c4dbd1e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
