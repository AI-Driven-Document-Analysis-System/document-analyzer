from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from typing import Any, Dict, List, Optional
import asyncio
import json


class StreamingCallbackHandler(BaseCallbackHandler):
    """
    A callback handler for streaming LLM responses via WebSocket connections.
    
    This class implements LangChain's BaseCallbackHandler interface to capture
    tokens as they are generated by the language model and stream them to
    connected WebSocket clients in real-time. It provides both token-by-token
    streaming and complete response delivery.
    
    The handler is designed for real-time chat applications where users expect
    immediate feedback as the AI generates responses.
    """
    
    def __init__(self, websocket=None, client_id: str = None):
        """
        Args:
            websocket: WebSocket connection object for streaming tokens
            client_id (str): Unique identifier for the client connection
        """
        self.websocket = websocket
        self.client_id = client_id
        # Store all generated tokens for complete response reconstruction
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """
        Handle new token generation from the language model.
        
        This method is called by LangChain whenever the LLM generates a new token.
        It stores the token and optionally streams it to the connected WebSocket client.
        
        Args:
            token (str): The newly generated token from the LLM
            **kwargs: Additional keyword arguments from LangChain
        """
        # Store the token for complete response reconstruction
        self.tokens.append(token)
        
        # Stream the token to WebSocket client if connection is available
        if self.websocket and self.client_id:
            asyncio.create_task(self._send_token(token))

    async def _send_token(self, token: str):
        """
        Send a single token to the connected WebSocket client.
        
        Args:
            token (str): The token to send to the client
        """
        if self.websocket:
            await self.websocket.send_text(json.dumps({
                'type': 'token',
                'data': token,
                'client_id': self.client_id
            }))

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """
        Handle completion of LLM response generation.
        
        This method is called by LangChain when the LLM finishes generating
        the complete response. It reconstructs the full response and sends it
        to the WebSocket client.
        
        Args:
            response (LLMResult): The complete LLM response object
            **kwargs: Additional keyword arguments from LangChain
        """
        # Reconstruct the complete response from all stored tokens
        full_response = "".join(self.tokens)
        
        # Send complete response to WebSocket client if connection is available
        if self.websocket and self.client_id:
            asyncio.create_task(self._send_complete(full_response))

    async def _send_complete(self, response: str):
        """
        Send the complete response to the connected WebSocket client.
        
        Args:
            response (str): The complete generated response
        """
        if self.websocket:
            await self.websocket.send_text(json.dumps({
                'type': 'complete',
                'data': response,
                'client_id': self.client_id
            }))


class AsyncStreamingCallbackHandler(BaseCallbackHandler):
    """
    An asynchronous callback handler for streaming LLM responses (Queue-based streaming).
    
    This class provides an async interface for streaming tokens without requiring
    a WebSocket connection. It uses an asyncio Queue to buffer tokens and provides
    an async generator interface for consuming tokens as they are generated.
    
    This handler is ideal for server-side streaming where tokens need to be
    processed asynchronously without direct WebSocket communication.
    """
    
    def __init__(self):
        # Store all generated tokens for complete response reconstruction
        self.tokens = []
        # Queue for buffering tokens as they are generated
        self.token_queue = asyncio.Queue()

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """
        Handle new token generation from the language model.
        
        This method is called by LangChain whenever the LLM generates a new token.
        It stores the token and adds it to the async queue for streaming.
        
        Args:
            token (str): The newly generated token from the LLM
            **kwargs: Additional keyword arguments from LangChain
        """
        # Store the token for complete response reconstruction
        self.tokens.append(token)
        # Add token to the async queue for streaming
        asyncio.create_task(self.token_queue.put(token))

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """
        Handle completion of LLM response generation.
        
        This method is called by LangChain when the LLM finishes generating
        the complete response. It signals the end of token generation by
        adding None to the queue.
        
        Args:
            response (LLMResult): The complete LLM response object
            **kwargs: Additional keyword arguments from LangChain
        """
        # Signal end of token generation by adding None to the queue
        asyncio.create_task(self.token_queue.put(None))

    async def get_tokens(self):
        """
        Async generator that yields tokens as they are generated.
        
        This method provides an async generator interface for consuming
        tokens as they are generated by the LLM. It yields tokens until
        the LLM finishes generating the complete response.
        
        Yields:
            str: Individual tokens as they are generated by the LLM
        """
        while True:
            # Wait for the next token from the queue
            token = await self.token_queue.get()
            
            # None indicates the end of token generation
            if token is None:
                break
                
            # Yield the token for processing
            yield token